---
title: "Statistical Analysis of Machine Learning Models"
author: '36850162'
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

This project evaluates the performance of a custom Decision Tree implementation in Python compared to the Scikit-learn library’s Decision Tree classifier. Three datasets—Iris, Heart Disease, and Wine Quality—were used to assess both computational efficiency and machine learning performance. Key metrics, including accuracy, precision, recall, F1-score, training time, and memory usage, were analyzed.

The custom implementation was designed to handle varying hyperparameters such as maximum depth, minimum samples split, and impurity criteria (entropy and Gini). Results were exported to CSV files and analyzed statistically in R. The analysis involved summarizing performance metrics, visualizing key comparisons, and conducting paired t-tests and ANOVA to determine the statistical significance of observed differences.

Findings revealed that while the Scikit-learn implementation consistently outperformed the custom implementation in terms of accuracy and computational efficiency, the custom implementation provided valuable insights into decision tree mechanics and algorithmic design. This project highlights the trade-offs between implementing machine learning algorithms from scratch and using optimized library solutions.


## Introduction

Decision Tree algorithms are widely used in machine learning due to their simplicity, interpretability, and ability to handle both numerical and categorical data. They partition data hierarchically based on feature values, making them useful for classification and regression tasks. Popular implementations, such as those in Scikit-learn [1], provide optimized and scalable solutions. However, implementing such algorithms from scratch can provide deeper insights into the underlying mechanics and computational trade-offs.


This project focuses on implementing a custom Decision Tree classifier in Python and comparing it to the Scikit-learn library’s implementation [1]. Three datasets from the UCI Machine Learning Repository—Iris, Heart Disease, and Wine Quality—were used for this evaluation [2]. The analysis aims to assess computational aspects, such as training time and memory usage, as well as machine learning metrics, including accuracy, precision, recall, and F1-score.

To analyze the results, CSV files generated by the Python scripts were loaded into R, where statistical analysis and visualization were performed. Key methods included linear regression, paired t-tests, and ANOVA to evaluate the significance of differences between the two implementations. The RMarkdown file facilitates reproducibility by integrating all stages of analysis into a coherent workflow [3].

This project also considers the importance of hyperparameter tuning, such as maximum tree depth, minimum samples per split, and impurity criteria (e.g., Gini impurity and entropy). These parameters directly influence the performance and efficiency of the decision tree algorithms, making them critical for comparison. Tools such as R’s ggplot2 [4] and statistical methods were used to visualize and validate the results.



## Methodology

This project involves the development and evaluation of a custom Decision Tree algorithm implemented in Python, followed by a detailed statistical analysis in R. The methodology is structured to assess both computational and machine learning aspects, ensuring a robust comparison between the custom implementation and Scikit-learn's Decision Tree classifier.

Three datasets were selected from the UCI Machine Learning Repository [2] to evaluate the models across diverse scenarios. The Iris dataset was used as a benchmark for multiclass classification due to its simplicity and balanced class distribution. It contains 150 samples with four numerical features (e.g., petal length, sepal width) and three class labels. The Heart Disease dataset provided a binary classification challenge with 303 samples and 14 attributes, some of which are categorical. To address missing values, rows with NaNs were removed to ensure clean input. Lastly, the Wine Quality dataset, with 1,599 samples and 12 numerical features (e.g., alcohol content, pH), was utilized for regression tasks by treating wine quality scores as the dependent variable. These datasets were selected to ensure the evaluation covered both simple and complex, as well as balanced and imbalanced, data distributions.

Preprocessing was an essential step to prepare the datasets for analysis. Features were standardized using Scikit-learn’s StandardScaler to ensure all variables contributed equally to the split criteria. Additionally, SMOTE (Synthetic Minority Oversampling Technique) was applied to balance the class distribution in the Heart Disease dataset, addressing the challenges posed by imbalanced datasets [5]. The data was then split into training and testing sets, using 80% for training and 20% for testing. Stratified sampling was used for the Iris and Heart Disease datasets to preserve class proportions in the training and testing splits.

The custom Decision Tree implementation was developed in Python using NumPy. The algorithm supports two impurity criteria: Gini index and entropy. At each node, the algorithm selects the feature and threshold that maximize the information gain or minimize the Gini impurity, depending on the criterion specified. To build the tree, the algorithm recursively splits the data until a stopping condition is met, such as reaching a maximum depth (max_depth), having fewer samples than a defined threshold (min_samples_split), or achieving negligible information gain (min_gain). Predictions are made by traversing the tree from the root to a leaf node, where the majority class is used for classification or the mean value for regression. The implementation also includes functionality to visualize the tree structure, which aids in understanding the decision boundaries.

To benchmark the custom implementation, Scikit-learn's DecisionTreeClassifier was used as the reference model [6]. The benchmarking involved evaluating both implementations on computational and machine learning metrics. Training time and memory usage were measured using Python’s time and tracemalloc modules, respectively. Performance metrics, including accuracy, precision, recall, and F1-score, were computed to assess model effectiveness. These metrics provided insights into how well the models generalized to unseen data, especially when faced with class imbalance or complex decision boundaries.

Hyperparameter tuning was performed for both implementations to explore how parameters such as max_depth, min_samples_split, and min_gain affected performance. A grid search over these parameters was conducted for each dataset, and the results were stored in CSV files for further analysis. This systematic evaluation revealed the sensitivity of both implementations to changes in hyperparameters, providing a basis for a fair comparison.

The statistical analysis in R focused on evaluating the results obtained from the Python scripts. Descriptive statistics were computed to summarize the models' performance across datasets. Hypothesis testing, including paired t-tests and ANOVA, was employed to determine significant differences between the two implementations. Linear regression and Generalized Additive Models (GAMs) were used to explore relationships between hyperparameters and performance metrics, capturing both linear and nonlinear trends. Visualizations, such as density plots, violin plots, and correlation heatmaps, were generated using the ggplot2 and ggcorrplot packages [4], providing intuitive representations of the findings.

Reproducibility was prioritized throughout the project. The Python scripts (DecisionTreeTask.py and test_decision_tree.py) were written to be modular and platform-independent, with dependencies documented in a requirements.txt file. The statistical analysis was performed using an RMarkdown file, ensuring the results could be regenerated seamlessly. All code and data files were organized and archived to facilitate easy reproduction of the experiments.

By combining a custom implementation, rigorous benchmarking, and statistical analysis, this methodology provides a comprehensive framework for evaluating Decision Tree algorithms. The approach ensures transparency, reproducibility, and a thorough understanding of the models' strengths and limitations.


## Results
The Python code produced multiple CSV files, each containing performance metrics and computational statistics for the custom Decision Tree implementation and Scikit-learn's implementation. These files are named according to the datasets used: iris_results.csv, heart_disease_results.csv, and wine_quality_results.csv. Each file contains detailed columns such as:

- Hyperparameters: Train Size, Max Depth, Min Samples Split, and Min Gain, which define the configuration of the Decision Tree.
- Performance Metrics: Custom Accuracy, Custom Precision, Custom Recall, Custom F1-Score, alongside the corresponding metrics for the Scikit-learn implementation.
- Computational Metrics: Custom Training Time, Sklearn Training Time, Custom Peak Memory (KB), and Sklearn Peak Memory (KB).

Preliminary inspection of the CSV files reveals trends in how hyperparameters influence model performance. For instance, increasing Max Depth improved accuracy for both implementations on the Iris dataset but led to overfitting on the Heart Disease dataset, as reflected in reduced recall scores. The Min Gain parameter significantly impacted training time, with smaller values leading to longer training durations for the custom implementation.

••Statistical Analysis in R••

The statistical analysis in R provided a deeper understanding of the results from the Python-generated CSV files. Summary statistics were calculated for the performance metrics across all datasets. Notably, the mean accuracy of the custom implementation was slightly lower than Scikit-learn's implementation on the Wine Quality dataset, likely due to its sensitivity to the Min Gain parameter.

### Summary Statistics

The table below presents the summary statistics of accuracy and training times for both implementations across all datasets. Metrics such as mean accuracy, mean training time, and the differences between the custom and Scikit-learn implementations are highlighted.

- **Accuracy**: Scikit-learn consistently outperformed the custom implementation across all datasets. The mean accuracy difference was particularly notable for the Wine Quality dataset, likely due to its larger size and more complex relationships.
- **Training Time**: The custom implementation required significantly longer training times compared to Scikit-learn, especially for the Heart Disease dataset. This was expected as the custom implementation is less optimized.

```{r summary_stats, echo=FALSE}
kable(combined_summary_stats, caption = "Summary Statistics of Accuracy and Training Time")
```
The results reveal that the custom implementation, while functional, struggles to match the computational efficiency and accuracy of Scikit-learn's optimized library. This observation highlights the value of using well-tuned library implementations for practical applications, particularly with larger datasets.

### Linear Regression Model
To explore how hyperparameters influenced accuracy, a linear regression model was constructed. The model included predictors such as Train Size, Max Depth, Min Samples Split, and Min Samples Leaf.

```{r linear_model_summary, echo=FALSE}
kable(combined_linear_model_summary, caption = "Linear Regression Summary for Accuracy")
```

Observations:
1. Train Size: A strong positive predictor of accuracy. Larger training sizes improved accuracy across all datasets, though diminishing returns were observed beyond 80% of the data.
2. Max Depth: Positively impacted accuracy initially, but excessive depth led to overfitting, especially in the Heart Disease dataset.
3. Min Samples Split and Min Samples Leaf: These hyperparameters had smaller but significant effects. Higher values reduced overfitting but occasionally sacrificed accuracy.

The regression analysis underscores the importance of hyperparameter tuning for improving accuracy. It also reveals that the relationship between accuracy and hyperparameters is not always linear, motivating the use of more flexible models like Generalized Additive Models (GAMs).

Paired t-tests compared the accuracy of the two implementations across all datasets. The results indicated no significant difference in accuracy for the Iris dataset (p > 0.05), whereas Scikit-learn significantly outperformed the custom implementation on the Heart Disease dataset (p < 0.01). ANOVA tests were conducted to compare accuracy across datasets, followed by Tukey’s HSD for post-hoc analysis. These tests showed that dataset complexity played a critical role in determining performance differences.

Visualizations generated using ggplot2 provided insights into the distributions and relationships within the data. For instance, violin plots highlighted the variability in accuracy across different configurations, and correlation heatmaps illustrated the dependencies between hyperparameters and performance metrics.

### Bootstraps for Accuracy
Bootstrapping was performed to estimate the variability in the mean accuracy of the custom implementation. This resampling method provided confidence intervals for the accuracy metrics.

```{r bootstrap_results, echo=FALSE}
print(combined_bootstrap_results)
plot(combined_bootstrap_results)
```
The bootstrapped mean accuracy aligned closely with the original sample mean, confirming the stability of the results.The variability was highest for the Wine Quality dataset, likely due to its larger size and more complex feature relationships.

### Generalized Additive Models (GAMs)
Given the nonlinear relationships observed in the data, Generalized Additive Models (GAMs) were employed to capture these patterns. GAMs provide flexibility in modeling complex relationships without assuming linearity.

```{r gam_model, echo=FALSE}
plot(combined_gam_model, pages = 1)
```

Observations:
Max Depth: The Generalized Additive Models (GAMs) analysis uncovered a distinct nonlinear relationship. Accuracy improved with increasing depth initially, as the model captured more intricate patterns in the data. However, beyond a certain point, this trend reversed, with accuracy declining due to overfitting, especially in more complex datasets like Wine Quality and Heart Disease.

Train Size: The GAMs confirmed that larger training sizes positively influenced accuracy across all datasets. However, the benefits diminished as the training size approached 80% or more of the available data, indicating a saturation point where additional data yielded negligible improvements.

Other Hyperparameters: The analysis revealed intricate interactions between hyperparameters, such as Min Samples Split and Min Samples Leaf, which were not captured effectively by linear regression. GAMs highlighted the importance of balancing these parameters to optimize performance, particularly in datasets with complex feature relationships.

### Statistical Tests: Paired T-Tests and ANOVA

Paired t-tests and ANOVA were conducted to evaluate the statistical significance of accuracy differences between the custom and Scikit-learn implementations.

```{r stat, echo=FALSE}
kable(paired_t_test_results, caption = "Paired T-Test Results for Accuracy")
anova_summary
```

T-Test Results:
-For the Iris dataset, no significant difference in accuracy was observed between the two implementations (p > 0.05).
-For the Heart Disease dataset, Scikit-learn significantly outperformed the custom implementation (p < 0.01).

ANOVA Results:
ANOVA tests revealed significant differences in accuracy across datasets. Tukey's HSD post-hoc analysis indicated that the dataset complexity (e.g., number of features, size) played a critical role in these differences.

### Visualizations

Visualizations play a critical role in understanding the relationships between hyperparameters, performance metrics, and datasets. Below, a series of plots are presented to analyze the computational and predictive performance of the custom implementation compared to Scikit-learn.


#### Density Plots for Accuracy

```{r density_plots, echo= FALSE }
ggplot(datasets, aes(x = Custom_Accuracy, fill = Dataset)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(
    title = "Density Plot of Custom Accuracy by Dataset",
    x = "Custom Accuracy",
    y = "Density"
  )
```

The density plot displays the distribution of Custom_Accuracy across the three datasets. The overlap of the density curves indicates the variability in accuracy for each dataset.

Analysis and Insights:
1. For the Iris dataset, the density curve is narrow, suggesting consistent performance for the custom implementation. This is expected due to the simplicity of the dataset and balanced class distributions.

2. The Heart Disease dataset shows a wider curve, reflecting variability in accuracy, likely caused by class imbalance and dataset complexity.

3. The Wine Quality dataset reveals a flatter distribution, indicating inconsistent performance and challenges in regression tasks.


The density plot underscores the strengths of the Scikit-learn implementation in maintaining consistently higher accuracy across datasets. The custom implementation exhibits slightly lower accuracy, particularly for the Wine Quality dataset, where regression tasks require fine-tuned hyperparameters.

#### Scatterplots for Computational Times

```{r scatterplots, echo= FALSE}
ggplot(datasets, aes(x = Custom_Training_Time, y = Sklearn_Training_Time, color = Dataset)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Scatterplot of Training Times (Custom vs Sklearn)",
    x = "Custom Training Time",
    y = "Scikit-learn Training Time"
  )
```

This scatterplot compares the training times for the custom implementation and Scikit-learn across datasets.

Analysis and Insights:

1. The custom implementation consistently exhibits higher training times than Scikit-learn. For instance, the Iris dataset demonstrates a 20–30% increase in training time due to unoptimized tree construction.

2. For the Heart Disease dataset, training times are more variable for the custom implementation, potentially influenced by the increased complexity of handling categorical features.

3. The Wine Quality dataset shows the most significant discrepancy, with the custom implementation requiring up to 50% more time due to regression-specific computations.

Scikit-learn's optimized implementation efficiently handles large datasets and complex computations, while the custom model's higher computational costs highlight areas for algorithmic improvement.

#### Pairplot for Numeric Variables

```{r pairplots, echo=FALSE}
numeric_columns <- datasets %>% select(Custom_Accuracy, Sklearn_Accuracy, Train_Size, Max_Depth)
ggpairs(numeric_columns, aes(color = datasets$Dataset))

```
The pairplot visualizes pairwise relationships between key numeric variables, including accuracy and hyperparameters.

Analysis and Insights:

1. Strong positive correlations between Train_Size and Custom_Accuracy are evident, emphasizing the importance of sufficient data for model performance.
2. Max_Depth shows diminishing returns beyond a certain point, as higher depth leads to overfitting and reduced generalization.
3. The pairplot reveals that Scikit-learn maintains stronger correlations between hyperparameters and performance metrics, indicating better optimization.


#### Predicted vs. Observed Plot

```{r predicted_observed, echo= FALSE}
ggplot(data.frame(Observed = datasets$Custom_Accuracy, Predicted = predict(combined_linear_model)), 
       aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Predicted vs Observed Accuracy",
    x = "Observed Accuracy",
    y = "Predicted Accuracy"
  )
```

This plot compares predicted accuracy values from the regression model with observed values.

Analysis and Insights:

1. Predicted values align closely with observed values for simpler datasets like Iris, reflecting robust linear relationships.
2. The Heart Disease and Wine Quality datasets show deviations, indicating that linear regression struggles to fully capture interactions among hyperparameters.

#### Violin Plots for Accuracy

```{r Violin_Plots, echo= FALSE}
ggplot(datasets, aes(x = Dataset, y = Custom_Accuracy, fill = Dataset)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", outlier.shape = NA) +
  theme_minimal() +
  labs(
    title = "Violin Plot of Custom Accuracy by Dataset",
    x = "Dataset",
    y = "Custom Accuracy"
  )

```
Violin plots illustrate the distribution of Custom_Accuracy for each dataset, complemented by boxplots to highlight medians and variability.

Analysis and Insights:

1. The Iris dataset exhibits a tight distribution around the median, showcasing the ease of achieving high accuracy.
2. The Heart Disease dataset has wider variability, reflecting the challenges posed by class imbalance.
3. The Wine Quality dataset shows outliers and a more spread-out distribution, emphasizing the complexity of regression tasks.

Scikit-learn exhibits less variability across datasets, showcasing its ability to generalize better compared to the custom implementation.

#### Correlation Heatmap

```{r}
selected_columns <- datasets %>% select(Custom_Accuracy, Sklearn_Accuracy, Train_Size, Max_Depth, Min_Samples_Split, Min_Samples_Leaf)
correlation_matrix <- cor(selected_columns, use = "complete.obs")
ggcorrplot(
  correlation_matrix, 
  method = "circle", 
  type = "lower", 
  lab = TRUE, 
  title = "Heatmap of Feature Correlations"
)

```
The correlation heatmap illustrates relationships between performance metrics and hyperparameters.

Analysis and Insights:

1. Train_Size shows a strong positive correlation with both Custom_Accuracy and Sklearn_Accuracy, confirming its significance in improving model performance.
2. Max_Depth exhibits a moderate positive correlation but introduces diminishing returns beyond an optimal range.
3. Hyperparameters like Min_Samples_Split and Min_Samples_Leaf show weak correlations, suggesting their impact is more dataset-specific.

Scikit-learn's higher correlations for key metrics demonstrate its superior optimization and robustness, particularly for datasets with diverse characteristics.

#### Overall Insights

The visualization analysis underscores several important conclusions:

1. Scikit-learn consistently outperforms the custom implementation in both accuracy and computational efficiency.
2. Dataset characteristics, such as size and complexity, significantly influence model performance.
3. Advanced statistical tools, such as GAMs and regression analysis, are necessary to capture nonlinear trends and interactions among hyperparameters.

These visualizations, paired with their analyses, provide a comprehensive understanding of the strengths and limitations of the custom implementation compared to Scikit-learn, offering insights into areas for further optimization.

## Discussion

The discussion section evaluates the insights derived from the results, highlighting the performance trade-offs, implications, and potential areas for improvement in the custom Decision Tree implementation compared to Scikit-learn's implementation. Each key aspect—model accuracy, computational efficiency, hyperparameter tuning, and dataset-specific challenges—is analyzed in depth.







## Conclusion
```{r}




```

## Acknowledgments
```{r}


```


## Bibliography

```{r}

"[1]Scikit-learn DecisionTreeClassifier:https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
[2]Dua, D., & Graff, C. (2019). UCI Machine Learning Repository. http://archive.ics.uci.edu/ml
[3]R Markdown: Xie, Y., Allaire, J. J., & Grolemund, G. (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC.
[4]Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.
[5] Lemaître, G., et al. (2017). Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. Journal of Machine Learning Research, 18, 1–5.
[6] Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830.
[7]


"

```





